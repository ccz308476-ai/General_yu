### TSFA模块总结

#### 1. **动机**

**现有方法的问题：**
- 传统Transformer架构倾向于计算所有查询-键对之间的注意力，这不可避免地导致冗余和无关信息之间的交互，最终降低解释准确性。[10]
- HSI通常包含各种复杂的地物覆盖类型，传统MHSA机制会在上下文特征提取过程中不可避免地引入意外噪声。[3]
- 如图1(b)所示，绿色框代表相对于红色查询标记（沥青）的不必要标记，如草地和沥青，这些无关标记可能引入噪声，潜在地损害分类准确性。[3]
- 现有基于Transformer的HSI分类模型主要关注空间或光谱信息单独建模，忽略了HSI数据内空间-光谱上下文的集成建模。[3]

**提出模块的目的：**
- 提出TSFA（Token Selective Fusion Attention）机制来替代传统MHSA机制，克服传统方法的局限性。[10]
- 在自注意力计算过程中选择性地关注最相关的标记，有效忽略最不相关的信息，提高解释准确性同时减少冗余计算。[10]
- 通过分组操作保留HSI三维立方体的数据特征，并使用3D卷积提取具有空间和光谱属性的标记。[10]

#### 2. **模块工作原理和核心思想**

**核心思想：**
TSFA通过光谱分组策略和标记选择机制，在空间-光谱自注意力融合过程中战略性地选择和组合最重要的标记，捕获最关键的上下文信息。[3][10]

**工作原理：**

1. **分组和3D卷积处理：**
   - 将输入特征P ∈ ℝ^(C×H×W)分割成g组，每组产生c×H×W的特征图
   - 通过3D点卷积（1×1×1）和3D深度卷积（1×3×3）生成相应的查询Q、键K和值V：[10][11]
   ```
   Qᵢ, Kᵢ, Vᵢ = Split(dwc₁ₓ₃ₓ₃(1×1×1(eᵢ)))
   ```

2. **注意力矩阵计算：**
   - 重塑和转置张量Q、K、V，调整维度为(H×W×g)×c
   - 通过Q和K的点积生成密集注意力矩阵：[11][12]
   ```
   A = QᵢKᵢᵀ/λ, 其中λ = √(C/h)
   ```

3. **标记选择机制：**
   - 应用标记选择机制识别注意力矩阵A中最相关的前k%元素
   - 创建矩阵Aₖ，保留前k%的元素用于激活，其余20%被掩码为0：[12]
   ```
   Aₖₚᵩ = {Aₚᵩ, 如果Aₚᵩ > T[p]ₖ; -∞, 其他情况}
   ```
   其中T[p]ₖ是每行中最高k%注意力值的下阈值向量

4. **选择性注意力计算：**
   - 通过softmax函数激活选择性注意力矩阵：[12]
   ```
   A* = Softmax(Aₖ)
   ```
   - 计算选择性注意力输出并添加残差连接：[13]
   ```
   P'ᵢ = Pᵢ + A*ᵢVᵢ
   ```

5. **多头融合：**
   - 连接各头的输出并通过1×1卷积聚合：[13]
   ```
   P' = 1×1(Concat(P'ᵢ))
   ```

**参数分析：**
- **选择率k的影响：**实验表明k=0.8时性能最佳，过低会导致信息不足，过高会引入冗余信息。[16][19]
- **分组数g的影响：**g=4时在所有数据集上达到最优分类性能，同时保持相对高效的计算复杂度。[19][22]

#### 3. **总结**

TSFA模块通过创新的标记选择机制有效解决了传统自注意力计算中的冗余信息干扰问题。该模块的核心贡献在于：

1. **保持数据特性：**通过分组操作和3D卷积保留HSI的空间-光谱三维特征。[10]

2. **智能标记选择：**通过选择最相关的标记进行注意力计算，有效减少无关信息的干扰，提高分类准确性。[10][12]

3. **计算效率提升：**相比传统全注意力机制，选择性注意力减少了冗余计算，在保持性能的同时提高了效率。[10]

4. **性能验证：**消融实验证明TSFTB模块显著提升网络分类性能，与KSFTB模块结合使用时表现出协同效应。[22][25]

TSFA模块的设计体现了"少即是多"的思想，通过精确选择最有价值的信息进行处理，实现了性能和效率的双重优化，为高光谱图像分类提供了新的解决思路。